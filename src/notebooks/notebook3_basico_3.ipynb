{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Notebook 3: Parsear Tags e Extrair Dados\n",
    "\n",
    "**Objetivo**: Usar XML parser para extrair dados estruturados das notícias\n",
    "\n",
    "**O que vamos fazer:**\n",
    "1. 🛠️ Configurar parser XML profissional\n",
    "2. 🔍 Parsear o RSS e extrair elementos\n",
    "3. 📋 Estruturar dados de cada notícia\n",
    "4. 🧹 Limpar e formatar informações\n",
    "5. 📊 Analisar dados extraídos\n",
    "6. 💾 Preparar dados para o CSV\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 5. Limpeza e Formatação dos Dados\n",
    "\n",
    "Vamos limpar e formatar os dados extraídos para melhor qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_dados_noticias(dados_brutos):\n",
    "    \"\"\"\n",
    "    Limpa e formata os dados extraídos\n",
    "    \"\"\"\n    print(\"🧹 Iniciando limpeza e formatação dos dados...\")\n    \n    dados_limpos = []\n    \n    for i, item in enumerate(dados_brutos, 1):\n        print(f\"   🔧 Limpando item {i}/{len(dados_brutos)}\", end=\"\\r\")\n        \n        item_limpo = item.copy()\n        \n        # Limpar título\n        if item_limpo['titulo']:\n            item_limpo['titulo'] = html.unescape(item_limpo['titulo'])\n            item_limpo['titulo'] = re.sub(r'\\s+', ' ', item_limpo['titulo']).strip()\n        \n        # Limpar descrição\n        if item_limpo['descricao']:\n            # Remover tags HTML da descrição\n            item_limpo['descricao'] = re.sub(r'<[^>]+>', '', item_limpo['descricao'])\n            item_limpo['descricao'] = html.unescape(item_limpo['descricao'])\n            item_limpo['descricao'] = re.sub(r'\\s+', ' ', item_limpo['descricao']).strip()\n        \n        # Formatar data\n        if item_limpo['data_publicacao']:\n            item_limpo['data_formatada'] = formatar_data_rss(item_limpo['data_publicacao'])\n        else:\n            item_limpo['data_formatada'] = None\n        \n        # Validar URL\n        if item_limpo['link']:\n            item_limpo['link_valido'] = item_limpo['link'].startswith(('http://', 'https://'))\n        else:\n            item_limpo['link_valido'] = False\n        \n        # Calcular estatísticas do item\n        item_limpo['tamanho_titulo'] = len(item_limpo['titulo']) if item_limpo['titulo'] else 0\n        item_limpo['tamanho_descricao'] = len(item_limpo['descricao']) if item_limpo['descricao'] else 0\n        \n        dados_limpos.append(item_limpo)\n    \n    print(f\"\\n✅ Limpeza concluída! {len(dados_limpos)} items processados\")\n    return dados_limpos\n\ndef formatar_data_rss(data_rss):\n    \"\"\"\n    Converte data RSS para formato brasileiro\n    \"\"\"\n    try:\n        # Formato típico: \"Wed, 08 Aug 2025 10:30:00 -0300\"\n        # Remover timezone para simplificar parsing\n        data_sem_tz = data_rss.rsplit(' ', 1)[0]\n        \n        # Tentar diferentes formatos\n        formatos = [\n            \"%a, %d %b %Y %H:%M:%S\",\n            \"%d %b %Y %H:%M:%S\",\n            \"%Y-%m-%d %H:%M:%S\"\n        ]\n        \n        for formato in formatos:\n            try:\n                data_obj = datetime.strptime(data_sem_tz, formato)\n                return data_obj.strftime(\"%d/%m/%Y %H:%M:%S\")\n            except ValueError:\n                continue\n        \n        # Se não conseguiu converter, retorna original\n        return data_rss\n        \n    except Exception:\n        return data_rss\n\n# Executar limpeza\nif dados_noticias:\n    dados_limpos = limpar_dados_noticias(dados_noticias)\nelse:\n    dados_limpos = []\n    print(\"⚠️ Nenhum dado para limpar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 6. Análise dos Dados Extraídos\n",
    "\n",
    "Vamos analisar os dados extraídos e gerar estatísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_dados_extraidos(dados):\n",
    "    \"\"\"\n",
    "    Analisa os dados extraídos e gera estatísticas\n",
    "    \"\"\"\n    if not dados:\n        print(\"❌ Nenhum dado para analisar\")\n        return None\n    \n    print(\"📊 ANÁLISE DOS DADOS EXTRAÍDOS\")\n    print(\"=\" * 40)\n    \n    total_items = len(dados)\n    \n    # Estatísticas básicas\n    print(f\"📈 ESTATÍSTICAS BÁSICAS:\")\n    print(f\"   Total de notícias: {total_items}\")\n    \n    # Completude dos dados\n    campos_principais = ['titulo', 'link', 'descricao', 'data_publicacao', 'categoria']\n    \n    print(f\"\\n✅ COMPLETUDE DOS DADOS:\")\n    for campo in campos_principais:\n        count_validos = sum(1 for item in dados if item.get(campo))\n        percentual = (count_validos / total_items) * 100\n        print(f\"   {campo.capitalize()}: {count_validos}/{total_items} ({percentual:.1f}%)\")\n    \n    # Análise de categorias\n    categorias = {}\n    for item in dados:\n        cat = item.get('categoria', 'Sem categoria')\n        if cat:\n            categorias[cat] = categorias.get(cat, 0) + 1\n    \n    print(f\"\\n📂 CATEGORIAS ENCONTRADAS ({len(categorias)} únicas):\")\n    for categoria, count in sorted(categorias.items(), key=lambda x: x[1], reverse=True)[:10]:\n        print(f\"   {categoria}: {count} notícias\")\n    \n    # Análise de tamanhos\n    tamanhos_titulo = [item['tamanho_titulo'] for item in dados if item.get('tamanho_titulo', 0) > 0]\n    tamanhos_descricao = [item['tamanho_descricao'] for item in dados if item.get('tamanho_descricao', 0) > 0]\n    \n    if tamanhos_titulo:\n        print(f\"\\n📏 ANÁLISE DE TAMANHOS:\")\n        print(f\"   Título médio: {sum(tamanhos_titulo) / len(tamanhos_titulo):.1f} caracteres\")\n        print(f\"   Título maior: {max(tamanhos_titulo)} caracteres\")\n        print(f\"   Título menor: {min(tamanhos_titulo)} caracteres\")\n    \n    if tamanhos_descricao:\n        print(f\"   Descrição média: {sum(tamanhos_descricao) / len(tamanhos_descricao):.1f} caracteres\")\n        print(f\"   Descrição maior: {max(tamanhos_descricao)} caracteres\")\n        print(f\"   Descrição menor: {min(tamanhos_descricao)} caracteres\")\n    \n    # Qualidade dos links\n    links_validos = sum(1 for item in dados if item.get('link_valido', False))\n    print(f\"\\n🔗 QUALIDADE DOS LINKS:\")\n    print(f\"   Links válidos: {links_validos}/{total_items} ({(links_validos/total_items)*100:.1f}%)\")\n    \n    return {\n        'total_items': total_items,\n        'categorias': categorias,\n        'completude': {campo: sum(1 for item in dados if item.get(campo)) for campo in campos_principais},\n        'links_validos': links_validos\n    }\n\n# Executar análise\nestats_dados = analisar_dados_extraidos(dados_limpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👀 7. Visualização de Amostras\n",
    "\n",
    "Vamos visualizar algumas amostras dos dados extraídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_amostras_dados(dados, quantidade=3):\n",
    "    \"\"\"\n",
    "    Exibe amostras dos dados extraídos\n",
    "    \"\"\"\n    if not dados:\n        print(\"❌ Nenhum dado para exibir\")\n        return\n    \n    print(f\"👀 AMOSTRAS DOS DADOS EXTRAÍDOS (primeiras {quantidade})\")\n    print(\"=\" * 60)\n    \n    for i, item in enumerate(dados[:quantidade], 1):\n        print(f\"\\n📰 NOTÍCIA {i}:\")\n        print(\"-\" * 40)\n        \n        # Informações principais\n        print(f\"🆔 ID: {item.get('id_item', 'N/A')}\")\n        print(f\"🏷️ Título: {item.get('titulo', 'N/A')[:80]}{'...' if len(item.get('titulo', '')) > 80 else ''}\")\n        print(f\"📅 Data: {item.get('data_formatada', 'N/A')} (original: {item.get('data_publicacao', 'N/A')[:25]}...)\")\n        print(f\"📂 Categoria: {item.get('categoria', 'N/A')}\")\n        print(f\"🔗 Link: {item.get('link', 'N/A')[:60]}{'...' if len(item.get('link', '')) > 60 else ''}\")\n        print(f\"📝 Descrição: {item.get('descricao', 'N/A')[:120]}{'...' if len(item.get('descricao', '')) > 120 else ''}\")\n        \n        # Metadados\n        print(f\"📊 Metadados:\")\n        print(f\"   Tamanho título: {item.get('tamanho_titulo', 0)} chars\")\n        print(f\"   Tamanho descrição: {item.get('tamanho_descricao', 0)} chars\")\n        print(f\"   Link válido: {'✅' if item.get('link_valido', False) else '❌'}\")\n        print(f\"   Elementos XML: {item.get('total_elementos', 0)}\")\n        print(f\"   Extraído em: {item.get('timestamp_extracao', 'N/A')}\")\n\n# Exibir amostras\nexibir_amostras_dados(dados_limpos, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 8. Preparação para CSV e Salvamento\n",
    "\n",
    "Vamos preparar os dados para serem salvos em CSV no próximo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_dados_para_csv(dados):\n",
    "    \"\"\"\n",
    "    Prepara os dados para serem salvos em CSV\n",
    "    Remove campos desnecessários e organiza campos principais\n",
    "    \"\"\"\n    if not dados:\n        return []\n    \n    print(\"🔄 Preparando dados para CSV...\")\n    \n    # Campos que queremos no CSV (ordem específica)\n    campos_csv = [\n        'id_item',\n        'titulo', \n        'data_publicacao',\n        'data_formatada',\n        'categoria',\n        'link',\n        'descricao',\n        'guid',\n        'tamanho_titulo',\n        'tamanho_descricao',\n        'link_valido',\n        'timestamp_extracao'\n    ]\n    \n    dados_csv = []\n    \n    for item in dados:\n        item_csv = {}\n        \n        # Copiar apenas campos necessários\n        for campo in campos_csv:\n            valor = item.get(campo)\n            \n            # Tratar valores None\n            if valor is None:\n                item_csv[campo] = \"\"\n            # Converter booleanos para texto\n            elif isinstance(valor, bool):\n                item_csv[campo] = \"Sim\" if valor else \"Não\"\n            # Garantir que é string\n            else:\n                item_csv[campo] = str(valor)\n        \n        dados_csv.append(item_csv)\n    \n    print(f\"✅ Dados preparados! {len(dados_csv)} registros prontos para CSV\")\n    print(f\"📋 Campos incluídos: {len(campos_csv)}\")\n    \n    return dados_csv, campos_csv\n\ndef salvar_dados_json_intermediario(dados, campos):\n    \"\"\"\n    Salva os dados em JSON como backup intermediário\n    \"\"\"\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    nome_arquivo = f\"dados_extraidos_{timestamp}.json\"\n    caminho = output_dir / nome_arquivo\n    \n    try:\n        dados_para_salvar = {\n            'metadados': {\n                'total_registros': len(dados),\n                'campos_incluidos': campos,\n                'timestamp_extracao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n                'fonte': 'Notebook 3 - Parsing de Tags'\n            },\n            'dados': dados\n        }\n        \n        with open(caminho, 'w', encoding='utf-8') as arquivo:\n            json.dump(dados_para_salvar, arquivo, ensure_ascii=False, indent=2)\n        \n        print(f\"💾 Backup JSON salvo: {nome_arquivo}\")\n        print(f\"📏 Tamanho: {caminho.stat().st_size:,} bytes\")\n        return str(caminho)\n        \n    except Exception as e:\n        print(f\"❌ Erro ao salvar JSON: {e}\")\n        return None\n\n# Preparar dados para CSV\nif dados_limpos:\n    dados_csv, campos_csv = preparar_dados_para_csv(dados_limpos)\n    \n    # Salvar backup JSON\n    arquivo_json = salvar_dados_json_intermediario(dados_csv, campos_csv)\nelse:\n    dados_csv, campos_csv = [], []\n    print(\"⚠️ Nenhum dado para preparar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 9. Resumo e Preparação para o Notebook 4\n",
    "\n",
    "Vamos fazer um resumo completo e preparar tudo para o próximo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 RESUMO COMPLETO DO NOTEBOOK 3\")\nprint(\"=\" * 50)\n\nprint(\"✅ TAREFAS REALIZADAS:\")\nprint(\"   🛠️ Configuração do parser XML profissional\")\nprint(\"   🔍 Parsing do RSS e extração de elementos\")\nprint(\"   📋 Estruturação de dados de cada notícia\")\nprint(\"   🧹 Limpeza e formatação de informações\")\nprint(\"   📊 Análise dos dados extraídos\")\nprint(\"   💾 Preparação dos dados para CSV\")\n\nif dados_csv:\n    print(f\"\\n📊 ESTATÍSTICAS FINAIS:\")\n    print(f\"   🗞️ Notícias processadas: {len(dados_csv)}\")\n    print(f\"   📋 Campos estruturados: {len(campos_csv)}\")\n    \n    if estats_dados:\n        print(f\"   📂 Categorias únicas: {len(estats_dados['categorias'])}\")\n        print(f\"   🔗 Links válidos: {estats_dados['links_validos']}\")\n    \n    print(f\"\\n📋 CAMPOS PREPARADOS PARA CSV:\")\n    for i, campo in enumerate(campos_csv, 1):\n        print(f\"   {i:2d}. {campo}\")\nelse:\n    print(\"\\n⚠️ Nenhum dado foi processado com sucesso\")\n\nprint(\"\\n📁 ARQUIVOS GERADOS:\")\nif arquivo_json:\n    print(f\"   📄 {Path(arquivo_json).name} - Dados estruturados em JSON\")\nelse:\n    print(\"   ❌ Nenhum arquivo gerado\")\n\nprint(\"\\n📚 O QUE APRENDEMOS:\")\nprint(\"   🔧 Como usar xml.etree.ElementTree para parsing\")\nprint(\"   🧹 Como limpar e formatar dados extraídos\")\nprint(\"   📊 Como analisar qualidade dos dados\")\nprint(\"   🏗️ Como estruturar dados para exportação\")\nprint(\"   💾 Como criar backups intermediários\")\n\nprint(\"\\n🎯 DADOS PRONTOS PARA O NOTEBOOK 4:\")\nprint(\"   📊 Dados estruturados e limpos\")\nprint(\"   📋 Campos organizados para CSV\")\nprint(\"   ✅ Validação de qualidade concluída\")\nprint(\"   💾 Backup de segurança criado\")\n\nprint(\"\\n\" + \"=\" * 50)\nif dados_csv:\n    print(\"🚀 Pronto para o Notebook 4 - Criação do CSV!\")\n    print(f\"💡 {len(dados_csv)} registros aguardando conversão para CSV\")\nelse:\n    print(\"⚠️ Verifique os erros acima antes de prosseguir\")\n    \nprint(f\"🕐 Notebook 3 concluído em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ 1. Configuração e Importações\n",
    "\n",
    "Vamos importar as bibliotecas necessárias, incluindo o parser XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "import re\n",
    "import html\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"✅ Bibliotecas importadas!\")\n",
    "print(f\"🕐 Notebook 3 iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"🔧 Parser XML: xml.etree.ElementTree\")\n",
    "\n",
    "# Criar diretório para outputs\n",
    "output_dir = Path(\"notebook_outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"📁 Diretório de saída: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 2. Capturando o Conteúdo RSS\n",
    "\n",
    "Vamos reutilizar nossa função de requisição dos notebooks anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capturar_rss(url):\n",
    "    \"\"\"\n",
    "    Captura conteúdo RSS - função consolidada dos notebooks anteriores\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'application/rss+xml, application/xml, text/xml',\n",
    "        'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"🔄 Capturando RSS de: {url}\")\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        print(f\"✅ RSS capturado! Tamanho: {len(response.text):,} caracteres\")\n",
    "        return response.text\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Erro ao capturar RSS: {e}\")\n",
    "        return None\n",
    "\n",
    "# URL do G1 RSS Brasil\n",
    "url_rss = \"https://g1.globo.com/rss/g1/brasil/\"\n",
    "\n",
    "# Capturar conteúdo\n",
    "conteudo_rss = capturar_rss(url_rss)\n",
    "\n",
    "if not conteudo_rss:\n",
    "    print(\"❌ Não foi possível continuar sem o conteúdo RSS\")\n",
    "else:\n",
    "    print(f\"📊 Pronto para parsing! Conteúdo: {len(conteudo_rss):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 3. Parser XML Profissional\n",
    "\n",
    "Vamos criar um parser XML robusto para extrair dados estruturados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsear_rss_xml(conteudo_xml):\n",
    "    \"\"\"\n",
    "    Parser XML profissional para RSS\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (sucesso, root_element, items_list, info_parsing)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🔄 Iniciando parsing XML...\")\n",
    "        \n",
    "        # Fazer parsing do XML\n",
    "        root = ET.fromstring(conteudo_xml)\n",
    "        \n",
    "        # Buscar elementos items\n",
    "        items = root.findall('.//item')\n",
    "        \n",
    "        # Informações do parsing\n",
    "        info_parsing = {\n",
    "            'root_tag': root.tag,\n",
    "            'total_items': len(items),\n",
    "            'namespaces': list(set([elem.tag.split('}')[0].strip('{') for elem in root.iter() if '}' in elem.tag])),\n",
    "            'parsing_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Parsing concluído!\")\n",
    "        print(f\"📊 Items encontrados: {len(items)}\")\n",
    "        print(f\"🏷️ Root tag: {root.tag}\")\n",
    "        \n",
    "        return True, root, items, info_parsing\n",
    "        \n",
    "    except ET.ParseError as e:\n",
    "        print(f\"❌ Erro de parsing XML: {e}\")\n",
    "        return False, None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro inesperado: {e}\")\n",
    "        return False, None, None, None\n",
    "\n",
    "# Executar parsing\n",
    "sucesso_parsing, root_xml, items_xml, info_parsing = parsear_rss_xml(conteudo_rss)\n",
    "\n",
    "if not sucesso_parsing:\n",
    "    print(\"❌ Falha no parsing XML. Não é possível continuar.\")\n",
    "else:\n",
    "    print(\"\\n📋 INFORMAÇÕES DO PARSING:\")\n",
    "    for key, value in info_parsing.items():\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 4. Extração Estruturada de Dados\n",
    "\n",
    "Vamos extrair dados de cada item de notícia de forma estruturada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_dados_item(item_xml, indice):\n",
    "    \"\"\"\n",
    "    Extrai dados estruturados de um item XML\n",
    "    \n",
    "    Args:\n",
    "        item_xml: Elemento XML do item\n",
    "        indice: Índice do item na lista\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dados estruturados do item\n",
    "    \"\"\"\n",
    "    dados = {\n",
    "        'id_item': indice,\n",
    "        'timestamp_extracao': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    # Elementos básicos do RSS\n",
    "    elementos_basicos = {\n",
    "        'titulo': 'title',\n",
    "        'link': 'link', \n",
    "        'descricao': 'description',
        'data_publicacao': 'pubDate',
        'categoria': 'category',
        'guid': 'guid'
    }
    
    # Extrair elementos básicos
    for campo_dados, tag_xml in elementos_basicos.items():
        elemento = item_xml.find(tag_xml)
        if elemento is not None and elemento.text:
            dados[campo_dados] = elemento.text.strip()
        else:
            dados[campo_dados] = None
    
    # Processamento especial para GUID (pode ter atributos)
    guid_elem = item_xml.find('guid')
    if guid_elem is not None:
        dados['guid'] = guid_elem.text.strip() if guid_elem.text else None
        dados['guid_permalink'] = guid_elem.get('isPermaLink', 'false')
    
    # Buscar elementos adicionais (namespaces)
    elementos_extras = item_xml.findall('.//*')
    dados['elementos_encontrados'] = [elem.tag for elem in elementos_extras]
    dados['total_elementos'] = len(elementos_extras)
    
    return dados

def processar_todos_items(items_xml):
    \"\"\"\n    Processa todos os items XML e extrai dados estruturados
    \"\"\"\n    print(f\"🔄 Processando {len(items_xml)} items...\")\n    \n    dados_extraidos = []\n    \n    for i, item in enumerate(items_xml, 1):\n        print(f\"   📋 Processando item {i}/{len(items_xml)}\", end=\"\\r\")\n        \n        dados_item = extrair_dados_item(item, i)\n        dados_extraidos.append(dados_item)\n    \n    print(f\"\\n✅ Processamento concluído! {len(dados_extraidos)} items processados\")\n    return dados_extraidos

# Processar todos os items
if sucesso_parsing:\n    dados_noticias = processar_todos_items(items_xml)\nelse:\n    dados_noticias = []