'categories': self.dados_brutos.count('<category>'),
                'pub_dates': self.dados_brutos.count('<pubDate>')
            }
            
            self.estatisticas['estrutura'] = elementos
            
            print(f"   üìä Estrutura analisada: {elementos['items']} not√≠cias encontradas")
            
            # Verificar se √© RSS v√°lido
            if elementos['items'] > 0 and '<?xml' in self.dados_brutos[:100]:
                print(f"   ‚úÖ RSS v√°lido detectado")
                return True
            else:
                print(f"   ‚ùå Estrutura RSS inv√°lida")
                return False
                
        except Exception as e:
            print(f"   ‚ùå Erro na an√°lise: {e}")
            return False
    
    def _parsear_e_extrair(self):
        """Etapa 3: Fazer parsing XML e extrair dados"""
        try:
            # Parsing XML
            root = ET.fromstring(self.dados_brutos)
            items = root.findall('.//item')
            
            print(f"   üîç XML parseado: {len(items)} items encontrados")
            
            # Extrair dados de cada item
            dados_extraidos = []
            for i, item in enumerate(items, 1):
                dados_item = self._extrair_dados_item(item, i)
                dados_extraidos.append(dados_item)
            
            self.dados_extraidos = dados_extraidos
            print(f"   ‚úÖ Dados extra√≠dos: {len(dados_extraidos)} registros")
            return True
            
        except ET.ParseError as e:
            print(f"   ‚ùå Erro de parsing XML: {e}")
            return False
        except Exception as e:
            print(f"   ‚ùå Erro na extra√ß√£o: {e}")
            return False
    
    def _extrair_dados_item(self, item_xml, indice):
        """Extrai dados de um item XML"""
        dados = {
            'id': indice,
            'timestamp_extracao': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # Elementos b√°sicos
        elementos = {
            'titulo': 'title',
            'link': 'link',
            'descricao': 'description',
            'data_publicacao': 'pubDate',
            'categoria': 'category',
            'guid': 'guid'
        }
        
        for campo, tag in elementos.items():
            elemento = item_xml.find(tag)
            dados[campo] = elemento.text.strip() if elemento is not None and elemento.text else None
        
        return dados
    
    def _limpar_e_preparar(self):
        """Etapa 4: Limpar e preparar dados"""
        print(f"   üßπ Limpando {len(self.dados_extraidos)} registros...")
        
        for item in self.dados_extraidos:
            item_limpo = item.copy()
            
            # Limpar t√≠tulo
            if item_limpo['titulo']:
                item_limpo['titulo'] = html.unescape(item_limpo['titulo'])
                item_limpo['titulo'] = re.sub(r'\\s+', ' ', item_limpo['titulo']).strip()
            
            # Limpar descri√ß√£o
            if item_limpo['descricao']:
                item_limpo['descricao'] = re.sub(r'<[^>]+>', '', item_limpo['descricao'])
                item_limpo['descricao'] = html.unescape(item_limpo['descricao'])
                item_limpo['descricao'] = re.sub(r'\\s+', ' ', item_limpo['descricao']).strip()
            
            # Formatar data
            if item_limpo['data_publicacao']:
                item_limpo['data_formatada'] = self._formatar_data(item_limpo['data_publicacao'])
            
            # Valida√ß√µes
            item_limpo['link_valido'] = item_limpo['link'] and item_limpo['link'].startswith(('http://', 'https://'))
            item_limpo['tamanho_titulo'] = len(item_limpo['titulo']) if item_limpo['titulo'] else 0
            item_limpo['tamanho_descricao'] = len(item_limpo['descricao']) if item_limpo['descricao'] else 0
            
            self.dados_limpos.append(item_limpo)
        
        print(f"   ‚úÖ Limpeza conclu√≠da: {len(self.dados_limpos)} registros limpos")
    
    def _formatar_data(self, data_rss):
        """Formata data do RSS para padr√£o brasileiro"""
        try:
            data_sem_tz = data_rss.rsplit(' ', 1)[0]
            data_obj = datetime.strptime(data_sem_tz, "%a, %d %b %Y %H:%M:%S")
            return data_obj.strftime("%d/%m/%Y %H:%M:%S")
        except:
            return data_rss

# Executar processo completo
scraper = G1ScraperCompleto()
sucesso_geral = scraper.executar_processo_completo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3. Cria√ß√£o do Arquivo CSV\n",
    "\n",
    "Agora vamos criar o arquivo CSV final com todos os dados estruturados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_csv_final(dados, diretorio_saida):\n",
    "    \"\"\"\n",
    "    Cria o arquivo CSV final com os dados extra√≠dos\n",
    "    \"\"\"\n    if not dados:\n        print(\"‚ùå Nenhum dado para criar CSV\")\n        return None\n    \n    print(\"üìä CRIANDO ARQUIVO CSV FINAL\")\n    print(\"-\" * 35)\n    \n    # Nome do arquivo com timestamp\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    nome_arquivo = f\"noticias_g1_brasil_{timestamp}.csv\"\n    caminho_arquivo = diretorio_saida / nome_arquivo\n    \n    # Definir campos do CSV\n    campos_csv = [\n        'id',\n        'titulo',\n        'data_publicacao',\n        'data_formatada', \n        'categoria',\n        'link',\n        'descricao',\n        'guid',\n        'link_valido',\n        'tamanho_titulo',\n        'tamanho_descricao',\n        'timestamp_extracao'\n    ]\n    \n    try:\n        print(f\"   üìù Criando arquivo: {nome_arquivo}\")\n        \n        with open(caminho_arquivo, 'w', newline='', encoding='utf-8') as arquivo:\n            writer = csv.DictWriter(arquivo, fieldnames=campos_csv)\n            \n            # Escrever cabe√ßalho\n            writer.writeheader()\n            \n            # Escrever dados\n            registros_escritos = 0\n            for item in dados:\n                # Preparar linha para CSV\n                linha_csv = {}\n                for campo in campos_csv:\n                    valor = item.get(campo, '')\n                    \n                    # Tratar valores especiais\n                    if valor is None:\n                        linha_csv[campo] = ''\n                    elif isinstance(valor, bool):\n                        linha_csv[campo] = 'Sim' if valor else 'N√£o'\n                    else:\n                        linha_csv[campo] = str(valor)\n                \n                writer.writerow(linha_csv)\n                registros_escritos += 1\n        \n        # Verificar arquivo criado\n        tamanho_arquivo = caminho_arquivo.stat().st_size\n        \n        print(f\"   ‚úÖ CSV criado com sucesso!\")\n        print(f\"   üìÅ Local: {caminho_arquivo}\")\n        print(f\"   üìä Registros: {registros_escritos:,}\")\n        print(f\"   üìè Tamanho: {tamanho_arquivo:,} bytes\")\n        print(f\"   üìã Campos: {len(campos_csv)}\")\n        \n        return {\n            'caminho': str(caminho_arquivo),\n            'nome': nome_arquivo,\n            'registros': registros_escritos,\n            'tamanho_bytes': tamanho_arquivo,\n            'campos': campos_csv\n        }\n        \n    except Exception as e:\n        print(f\"   ‚ùå Erro ao criar CSV: {e}\")\n        return None\n\n# Criar CSV se temos dados\nif sucesso_geral and scraper.dados_limpos:\n    info_csv = criar_csv_final(scraper.dados_limpos, final_dir)\nelse:\n    info_csv = None\n    print(\"‚ö†Ô∏è N√£o foi poss√≠vel criar CSV - verifique se o processo anterior foi bem-sucedido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã 4. Relat√≥rio de Qualidade dos Dados\n",
    "\n",
    "Vamos gerar um relat√≥rio detalhado sobre a qualidade dos dados extra√≠dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_relatorio_qualidade(dados, info_requisicao):\n    \"\"\"\n    Gera relat√≥rio detalhado de qualidade dos dados\n    \"\"\"\n    if not dados:\n        return None\n    \n    print(\"üìã RELAT√ìRIO DE QUALIDADE DOS DADOS\")\n    print(\"=\" * 45)\n    \n    total_registros = len(dados)\n    \n    # 1. Completude dos dados\n    print(\"\\n1Ô∏è‚É£ COMPLETUDE DOS DADOS:\")\n    campos_principais = ['titulo', 'link', 'descricao', 'data_publicacao', 'categoria']\n    \n    completude = {}\n    for campo in campos_principais:\n        completos = sum(1 for item in dados if item.get(campo) and str(item[campo]).strip())\n        percentual = (completos / total_registros) * 100\n        completude[campo] = {'completos': completos, 'percentual': percentual}\n        \n        status = \"‚úÖ\" if percentual >= 80 else \"‚ö†Ô∏è\" if percentual >= 50 else \"‚ùå\"\n        print(f\"   {status} {campo.capitalize()}: {completos}/{total_registros} ({percentual:.1f}%)\")\n    \n    # 2. Qualidade dos links\n    print(\"\\n2Ô∏è‚É£ QUALIDADE DOS LINKS:\")\n    links_validos = sum(1 for item in dados if item.get('link_valido', False))\n    perc_links = (links_validos / total_registros) * 100\n    status_links = \"‚úÖ\" if perc_links >= 90 else \"‚ö†Ô∏è\" if perc_links >= 70 else \"‚ùå\"\n    print(f\"   {status_links} Links v√°lidos: {links_validos}/{total_registros} ({perc_links:.1f}%)\")\n    \n    # 3. An√°lise de tamanhos\n    print(\"\\n3Ô∏è‚É£ AN√ÅLISE DE TAMANHOS:\")\n    tamanhos_titulo = [item['tamanho_titulo'] for item in dados if item.get('tamanho_titulo', 0) > 0]\n    tamanhos_desc = [item['tamanho_descricao'] for item in dados if item.get('tamanho_descricao', 0) > 0]\n    \n    if tamanhos_titulo:\n        media_titulo = sum(tamanhos_titulo) / len(tamanhos_titulo)\n        print(f\"   üìè T√≠tulos - M√©dia: {media_titulo:.1f} chars, Min: {min(tamanhos_titulo)}, Max: {max(tamanhos_titulo)}\")\n    \n    if tamanhos_desc:\n        media_desc = sum(tamanhos_desc) / len(tamanhos_desc)\n        print(f\"   üìè Descri√ß√µes - M√©dia: {media_desc:.1f} chars, Min: {min(tamanhos_desc)}, Max: {max(tamanhos_desc)}\")\n    \n    # 4. Distribui√ß√£o de categorias\n    print(\"\\n4Ô∏è‚É£ DISTRIBUI√á√ÉO DE CATEGORIAS:\")\n    categorias = {}\n    for item in dados:\n        cat = item.get('categoria') or 'Sem categoria'\n        categorias[cat] = categorias.get(cat, 0) + 1\n    \n    print(f\"   üìÇ Total de categorias √∫nicas: {len(categorias)}\")\n    \n    # Top 5 categorias\n    top_categorias = sorted(categorias.items(), key=lambda x: x[1], reverse=True)[:5]\n    for i, (cat, count) in enumerate(top_categorias, 1):\n        perc = (count / total_registros) * 100\n        print(f\"   {i}. {cat}: {count} ({perc:.1f}%)\")\n    \n    # 5. Score geral de qualidade\n    print(\"\\n5Ô∏è‚É£ SCORE GERAL DE QUALIDADE:\")\n    \n    # Calcular score baseado em diferentes crit√©rios\n    scores = {\n        'completude_titulo': completude['titulo']['percentual'],\n        'completude_link': completude['link']['percentual'], \n        'completude_descricao': completude['descricao']['percentual'],\n        'qualidade_links': perc_links,\n        'diversidade_categorias': min(100, len(categorias) * 10)  # Max 100\n    }\n    \n    score_final = sum(scores.values()) / len(scores)\n    \n    print(f\"   üèÜ Score Final: {score_final:.1f}/100\")\n    \n    if score_final >= 80:\n        print(f\"   üåü EXCELENTE! Dados de alta qualidade\")\n        qualidade = \"Excelente\"\n    elif score_final >= 60:\n        print(f\"   üëç BOM! Dados de qualidade adequada\")\n        qualidade = \"Boa\"\n    elif score_final >= 40:\n        print(f\"   ‚ö†Ô∏è REGULAR! Dados precisam de melhorias\")\n        qualidade = \"Regular\"\n    else:\n        print(f\"   ‚ùå BAIXA! Dados com problemas significativos\")\n        qualidade = \"Baixa\"\n    \n    # 6. Informa√ß√µes t√©cnicas\n    print(\"\\n6Ô∏è‚É£ INFORMA√á√ïES T√âCNICAS:\")\n    print(f\"   üåê URL fonte: {info_requisicao.get('url', 'N/A')}\")\n    print(f\"   ‚è±Ô∏è Tempo de captura: {info_requisicao.get('tempo_resposta', 0):.3f}s\")\n    print(f\"   üìä Tamanho original: {info_requisicao.get('tamanho_caracteres', 0):,} chars\")\n    print(f\"   üî§ Encoding: {info_requisicao.get('encoding', 'N/A')}\")\n    print(f\"   üïê Processado em: {info_requisicao.get('timestamp', 'N/A')}\")\n    \n    return {\n        'score_final': score_final,\n        'qualidade': qualidade,\n        'completude': completude,\n        'total_registros': total_registros,\n        'categorias': categorias,\n        'scores_detalhados': scores\n    }\n\n# Gerar relat√≥rio se temos dados\nif sucesso_geral and scraper.dados_limpos:\n    relatorio_qualidade = gerar_relatorio_qualidade(scraper.dados_limpos, scraper.info_requisicao)\nelse:\n    relatorio_qualidade = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 5. Visualiza√ß√µes dos Dados\n",
    "\n",
    "Vamos criar algumas visualiza√ß√µes simples usando caracteres para mostrar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_visualizacoes_dados(dados, relatorio):\n    \"\"\"\n    Cria visualiza√ß√µes simples dos dados usando caracteres\n    \"\"\"\n    if not dados or not relatorio:\n        return\n    \n    print(\"üìà VISUALIZA√á√ïES DOS DADOS\")\n    print(\"=\" * 35)\n    \n    # 1. Gr√°fico de completude\n    print(\"\\n1Ô∏è‚É£ GR√ÅFICO DE COMPLETUDE DOS CAMPOS:\")\n    print(\"-\" * 40)\n    \n    for campo, info in relatorio['completude'].items():\n        percentual = info['percentual']\n        barra_tamanho = int(percentual / 5)  # Cada char = 5%\n        barra = \"‚ñà\" * barra_tamanho + \"‚ñë\" * (20 - barra_tamanho)\n        \n        print(f\"{campo.capitalize():12} |{barra}| {percentual:5.1f}%\")\n    \n    # 2. Distribui√ß√£o de categorias (top 10)\n    print(\"\\n2Ô∏è‚É£ DISTRIBUI√á√ÉO DE CATEGORIAS (TOP 10):\")\n    print(\"-\" * 45)\n    \n    categorias = relatorio['categorias']\n    top_categorias = sorted(categorias.items(), key=lambda x: x[1], reverse=True)[:10]\n    max_count = max([count for _, count in top_categorias]) if top_categorias else 1\n    \n    for i, (categoria, count) in enumerate(top_categorias, 1):\n        # Truncar categoria se muito longa\n        cat_display = categoria[:15] + \"...\" if len(categoria) > 18 else categoria\n        \n        # Criar barra proporcional\n        barra_tamanho = int((count / max_count) * 25)\n        barra = \"‚ñì\" * barra_tamanho\n        \n        percentual = (count / relatorio['total_registros']) * 100\n        print(f\"{i:2d}. {cat_display:18} |{barra:<25}| {count:3d} ({percentual:4.1f}%)\")\n    \n    # 3. Indicadores de qualidade\n    print(\"\\n3Ô∏è‚É£ INDICADORES DE QUALIDADE:\")\n    print(\"-\" * 35)\n    \n    indicadores = [\n        (\"Score Geral\", relatorio['score_final'], 100),\n        (\"Completude T√≠tulos\", relatorio['completude']['titulo']['percentual'], 100),\n        (\"Completude Links\", relatorio['completude']['link']['percentual'], 100),\n        (\"Qualidade Links\", relatorio['scores_detalhados']['qualidade_links'], 100),\n        (\"Diversidade Cats\", relatorio['scores_detalhados']['diversidade_categorias'], 100)\n    ]\n    \n    for nome, valor, maximo in indicadores:\n        percentual = (valor / maximo) * 100\n        \n        # Escolher cor/s√≠mbolo baseado no valor\n        if percentual >= 80:\n            simbolo, cor = \"üü¢\", \"√ìTIMO\"\n        elif percentual >= 60:\n            simbolo, cor = \"üü°\", \"BOM\"\n        elif percentual >= 40:\n            simbolo, cor = \"üü†\", \"REGULAR\"\n        else:\n            simbolo, cor = \"üî¥\", \"BAIXO\"\n        \n        barra_tamanho = int(percentual / 5)\n        barra = \"‚ñà\" * barra_tamanho + \"‚ñë\" * (20 - barra_tamanho)\n        \n        print(f\"{simbolo} {nome:15} |{barra}| {valor:5.1f} ({cor})\")\n    \n    # 4. Timeline das publica√ß√µes (se tivermos datas)\n    print(\"\\n4Ô∏è‚É£ RESUMO TEMPORAL:\")\n    print(\"-\" * 25)\n    \n    datas_validas = []\n    for item in dados:\n        if item.get('data_formatada'):\n            try:\n                # Tentar extrair apenas a data\n                data_str = item['data_formatada'].split(' ')[0]\n                datas_validas.append(data_str)\n            except:\n                pass\n    \n    if datas_validas:\n        datas_unicas = list(set(datas_validas))\n        print(f\"   üìÖ Per√≠odo: {len(datas_unicas)} dias √∫nicos\")\n        print(f\"   üìä Not√≠cias/dia: {len(dados) / len(datas_unicas):.1f} (m√©dia)\")\n        \n        # Contar por data\n        contador_datas = {}\n        for data in datas_validas:\n            contador_datas[data] = contador_datas.get(data, 0) + 1\n        \n        # Mostrar top 5 dias\n        top_dias = sorted(contador_datas.items(), key=lambda x: x[1], reverse=True)[:5]\n        print(f\"\\n   üèÜ TOP 5 DIAS COM MAIS NOT√çCIAS:\")\n        for i, (data, count) in enumerate(top_dias, 1):\n            print(f\"   {i}. {data}: {count} not√≠cias\")\n    else:\n        print(\"   ‚ö†Ô∏è Nenhuma data v√°lida encontrada para an√°lise temporal\")\n\n# Criar visualiza√ß√µes se temos dados\nif relatorio_qualidade:\n    criar_visualizacoes_dados(scraper.dados_limpos, relatorio_qualidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 6. Exporta√ß√£o em M√∫ltiplos Formatos\n",
    "\n",
    "Vamos exportar os dados em diferentes formatos para m√°xima compatibilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportar_multiplos_formatos(dados, info_requisicao, relatorio_qualidade, diretorio):\n    \"\"\"\n    Exporta dados em m√∫ltiplos formatos\n    \"\"\"\n    if not dados:\n        return {}\n    \n    print(\"üíæ EXPORTANDO EM M√öLTIPLOS FORMATOS\")\n    print(\"-\" * 40)\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    arquivos_gerados = {}\n    \n    # 1. JSON detalhado\n    print(\"   üìÑ Criando JSON detalhado...\")\n    try:\n        nome_json = f\"dados_completos_{timestamp}.json\"\n        caminho_json = diretorio / nome_json\n        \n        dados_json = {\n            'metadados': {\n                'fonte': 'G1 RSS Brasil',\n                'url': info_requisicao.get('url'),\n                'timestamp_extracao': info_requisicao.get('timestamp'),\n                'total_registros': len(dados),\n                'qualidade_dados': relatorio_qualidade['qualidade'] if relatorio_qualidade else 'N/A',\n                'score_qualidade': relatorio_qualidade['score_final'] if relatorio_qualidade else 0,\n                'gerado_por': 'Notebook 4 - Scraping G1'\n            },\n            'dados': dados,\n            'estatisticas': relatorio_qualidade if relatorio_qualidade else {},\n            'info_tecnica': info_requisicao\n        }\n        \n        with open(caminho_json, 'w', encoding='utf-8') as arquivo:\n            json.dump(dados_json, arquivo, ensure_ascii=False, indent=2)\n        \n        arquivos_gerados['json'] = {\n            'nome': nome_json,\n            'caminho': str(caminho_json),\n            'tamanho': caminho_json.stat().st_size\n        }\n        print(f\"   ‚úÖ JSON: {nome_json} ({arquivos_gerados['json']['tamanho']:,} bytes)\")\n        \n    except Exception as e:\n        print(f\"   ‚ùå Erro no JSON: {e}\")\n    \n    # 2. TXT relat√≥rio\n    print(\"   üìÑ Criando relat√≥rio TXT...\")\n    try:\n        nome_txt = f\"relatorio_scraping_{timestamp}.txt\"\n        caminho_txt = diretorio / nome_txt\n        \n        with open(caminho_txt, 'w', encoding='utf-8') as arquivo:\n            arquivo.write(\"=\" * 60 + \"\\n\")\n            arquivo.write(\"RELAT√ìRIO DE SCRAPING - G1 RSS BRASIL\\n\")\n            arquivo.write(\"Gerado pelos Notebooks 1-4\\n\")\n            arquivo.write(\"=\" * 60 + \"\\n\\n\")\n            \n            # Informa√ß√µes gerais\n            arquivo.write(\"INFORMA√á√ïES GERAIS:\\n\")\n            arquivo.write(\"-\" * 20 + \"\\n\")\n            arquivo.write(f\"URL fonte: {info_requisicao.get('url', 'N/A')}\\n\")\n            arquivo.write(f\"Data/hora extra√ß√£o: {info_requisicao.get('timestamp', 'N/A')}\\n\")\n            arquivo.write(f\"Total de not√≠cias: {len(dados)}\\n\")\n            arquivo.write(f\"Tempo de resposta: {info_requisicao.get('tempo_resposta', 0):.3f}s\\n\")\n            \n            if relatorio_qualidade:\n                arquivo.write(f\"\\nQUALIDADE DOS DADOS:\\n\")\n                arquivo.write(\"-\" * 20 + \"\\n\")\n                arquivo.write(f\"Score geral: {relatorio_qualidade['score_final']:.1f}/100\\n\")\n                arquivo.write(f\"Classifica√ß√£o: {relatorio_qualidade['qualidade']}\\n\")\n                \n                arquivo.write(f\"\\nCOMPLETUDE POR CAMPO:\\n\")\n                for campo, info in relatorio_qualidade['completude'].items():\n                    arquivo.write(f\"  {campo}: {info['percentual']:.1f}%\\n\")\n                \n                arquivo.write(f\"\\nTOP 10 CATEGORIAS:\\n\")\n                categorias = relatorio_qualidade['categorias']\n                top_cats = sorted(categorias.items(), key=lambda x: x[1], reverse=True)[:10]\n                for i, (cat, count) in enumerate(top_cats, 1):\n                    perc = (count / len(dados)) * 100\n                    arquivo.write(f\"  {i:2d}. {cat}: {count} ({perc:.1f}%)\\n\")\n            \n            arquivo.write(f\"\\nPRIMEIRAS 5 NOT√çCIAS (AMOSTRA):\\n\")\n            arquivo.write(\"-\" * 35 + \"\\n\")\n            for i, item in enumerate(dados[:5], 1):\n                arquivo.write(f\"\\n{i}. {item.get('titulo', 'N/A')}\\n\")\n                arquivo.write(f\"   Data: {item.get('data_formatada', 'N/A')}\\n\")\n                arquivo.write(f\"   Categoria: {item.get('categoria', 'N/A')}\\n\")\n                arquivo.write(f\"   Link: {item.get('link', 'N/A')[:60]}...\\n\")\n            \n            arquivo.write(f\"\\n\\nFIM DO RELAT√ìRIO\\n\")\n        \n        arquivos_gerados['txt'] = {\n            'nome': nome_txt,\n            'caminho': str(caminho_txt),\n            'tamanho': caminho_txt.stat().st_size\n        }\n        print(f\"   ‚úÖ TXT: {nome_txt} ({arquivos_gerados['txt']['tamanho']:,} bytes)\")\n        \n    except Exception as e:\n        print(f\"   ‚ùå Erro no TXT: {e}\")\n    \n    return arquivos_gerados\n\n# Exportar se temos dados\nif sucesso_geral and scraper.dados_limpos:\n    arquivos_exportados = exportar_multiplos_formatos(\n        scraper.dados_limpos, \n        scraper.info_requisicao, \n        relatorio_qualidade, \n        final_dir\n    )\nelse:\n    arquivos_exportados = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 7. Relat√≥rio Final Completo\n",
    "\n",
    "Vamos gerar o relat√≥rio final de todo o projeto dos 4 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ RELAT√ìRIO FINAL COMPLETO DO PROJETO\")\nprint(\"=\" * 50)\n\n# Tempo total de execu√ß√£o\ntempo_total = datetime.now() - scraper.timestamp_inicio\n\nprint(f\"‚è±Ô∏è TEMPO DE EXECU√á√ÉO: {tempo_total.total{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Notebook 4: Criar CSV com Dados Finais\n",
    "\n",
    "**Objetivo**: Criar arquivo CSV final com todos os dados extra√≠dos e estruturados\n",
    "\n",
    "**O que vamos fazer:**\n",
    "1. üîÑ Consolidar todo o processo (Notebooks 1-3)\n",
    "2. üìä Criar CSV com dados estruturados\n",
    "3. üìã Gerar relat√≥rio final de qualidade\n",
    "4. üìà Criar visualiza√ß√µes dos dados\n",
    "5. üíæ Exportar m√∫ltiplos formatos\n",
    "6. üéØ Relat√≥rio completo do projeto\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 1. Configura√ß√£o Final e Importa√ß√µes\n",
    "\n",
    "Vamos importar tudo que precisamos para finalizar nosso projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√µes completas\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"üéâ NOTEBOOK 4 - CRIA√á√ÉO DO CSV FINAL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Todas as bibliotecas importadas!\")\n",
    "print(f\"üïê Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Configurar diret√≥rios\n",
    "output_dir = Path(\"notebook_outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "final_dir = Path(\"dados_finais\")\n",
    "final_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Diret√≥rio de trabalho: {output_dir}\")\n",
    "print(f\"üìÅ Diret√≥rio final: {final_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 2. Processo Completo Consolidado\n",
    "\n",
    "Vamos executar todo o processo de uma vez, consolidando os Notebooks 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G1ScraperCompleto:\n",
    "    \"\"\"\n",
    "    Classe que consolida todo o processo dos notebooks anteriores\n",
    "    \"\"\"\n    \n    def __init__(self):\n        self.url = \"https://g1.globo.com/rss/g1/brasil/\"\n        self.dados_brutos = None\n        self.dados_limpos = []\n        self.info_requisicao = {}\n        self.estatisticas = {}\n        self.timestamp_inicio = datetime.now()\n    \n    def executar_processo_completo(self):\n        \"\"\"\n        Executa todo o processo do scraping\n        \"\"\"\n        print(\"üöÄ INICIANDO PROCESSO COMPLETO DE SCRAPING\")\n        print(\"-\" * 45)\n        \n        # Etapa 1: Requisi√ß√£o (Notebook 1)\n        print(\"\\nüì° ETAPA 1: REQUISI√á√ÉO E CAPTURA\")\n        sucesso = self._fazer_requisicao()\n        if not sucesso:\n            return False\n        \n        # Etapa 2: An√°lise da estrutura (Notebook 2)\n        print(\"\\nüìä ETAPA 2: AN√ÅLISE DA ESTRUTURA\")\n        sucesso = self._analisar_estrutura()\n        if not sucesso:\n            return False\n        \n        # Etapa 3: Parsing e extra√ß√£o (Notebook 3)\n        print(\"\\nüîç ETAPA 3: PARSING E EXTRA√á√ÉO\")\n        sucesso = self._parsear_e_extrair()\n        if not sucesso:\n            return False\n        \n        # Etapa 4: Limpeza e prepara√ß√£o\n        print(\"\\nüßπ ETAPA 4: LIMPEZA E PREPARA√á√ÉO\")\n        self._limpar_e_preparar()\n        \n        print(\"\\n‚úÖ PROCESSO COMPLETO FINALIZADO!\")\n        return True\n    \n    def _fazer_requisicao(self):\n        \"\"\"Etapa 1: Fazer requisi√ß√£o HTTP\"\"\"\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Accept': 'application/rss+xml, application/xml, text/xml'\n        }\n        \n        try:\n            print(f\"   üîÑ Requisi√ß√£o para: {self.url}\")\n            response = requests.get(self.url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            self.dados_brutos = response.text\n            self.info_requisicao = {\n                'url': self.url,\n                'status_code': response.status_code,\n                'tamanho_bytes': len(response.content),\n                'tamanho_caracteres': len(response.text),\n                'encoding': response.encoding,\n                'tempo_resposta': response.elapsed.total_seconds(),\n                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            }\n            \n            print(f\"   ‚úÖ Sucesso! {len(self.dados_brutos):,} caracteres capturados\")\n            return True\n            \n        except requests.exceptions.RequestException as e:\n            print(f\"   ‚ùå Erro na requisi√ß√£o: {e}\")\n            return False\n    \n    def _analisar_estrutura(self):\n        \"\"\"Etapa 2: Analisar estrutura do RSS\"\"\"\n        try:\n            # Contar elementos principais\n            elementos = {\n                'items': self.dados_brutos.count('<item>'),\n                'titles': self.dados_brutos.count('<title>'),\n                'links': self.dados_brutos.count('<link>'),\n                'descriptions': self.dados_brutos.count('<description>'),\n                'categories': self.dados_brutos.count('