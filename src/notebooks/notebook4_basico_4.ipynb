'categories': self.dados_brutos.count('<category>'),
                'pub_dates': self.dados_brutos.count('<pubDate>')
            }
            
            self.estatisticas['estrutura'] = elementos
            
            print(f"   📊 Estrutura analisada: {elementos['items']} notícias encontradas")
            
            # Verificar se é RSS válido
            if elementos['items'] > 0 and '<?xml' in self.dados_brutos[:100]:
                print(f"   ✅ RSS válido detectado")
                return True
            else:
                print(f"   ❌ Estrutura RSS inválida")
                return False
                
        except Exception as e:
            print(f"   ❌ Erro na análise: {e}")
            return False
    
    def _parsear_e_extrair(self):
        """Etapa 3: Fazer parsing XML e extrair dados"""
        try:
            # Parsing XML
            root = ET.fromstring(self.dados_brutos)
            items = root.findall('.//item')
            
            print(f"   🔍 XML parseado: {len(items)} items encontrados")
            
            # Extrair dados de cada item
            dados_extraidos = []
            for i, item in enumerate(items, 1):
                dados_item = self._extrair_dados_item(item, i)
                dados_extraidos.append(dados_item)
            
            self.dados_extraidos = dados_extraidos
            print(f"   ✅ Dados extraídos: {len(dados_extraidos)} registros")
            return True
            
        except ET.ParseError as e:
            print(f"   ❌ Erro de parsing XML: {e}")
            return False
        except Exception as e:
            print(f"   ❌ Erro na extração: {e}")
            return False
    
    def _extrair_dados_item(self, item_xml, indice):
        """Extrai dados de um item XML"""
        dados = {
            'id': indice,
            'timestamp_extracao': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # Elementos básicos
        elementos = {
            'titulo': 'title',
            'link': 'link',
            'descricao': 'description',
            'data_publicacao': 'pubDate',
            'categoria': 'category',
            'guid': 'guid'
        }
        
        for campo, tag in elementos.items():
            elemento = item_xml.find(tag)
            dados[campo] = elemento.text.strip() if elemento is not None and elemento.text else None
        
        return dados
    
    def _limpar_e_preparar(self):
        """Etapa 4: Limpar e preparar dados"""
        print(f"   🧹 Limpando {len(self.dados_extraidos)} registros...")
        
        for item in self.dados_extraidos:
            item_limpo = item.copy()
            
            # Limpar título
            if item_limpo['titulo']:
                item_limpo['titulo'] = html.unescape(item_limpo['titulo'])
                item_limpo['titulo'] = re.sub(r'\\s+', ' ', item_limpo['titulo']).strip()
            
            # Limpar descrição
            if item_limpo['descricao']:
                item_limpo['descricao'] = re.sub(r'<[^>]+>', '', item_limpo['descricao'])
                item_limpo['descricao'] = html.unescape(item_limpo['descricao'])
                item_limpo['descricao'] = re.sub(r'\\s+', ' ', item_limpo['descricao']).strip()
            
            # Formatar data
            if item_limpo['data_publicacao']:
                item_limpo['data_formatada'] = self._formatar_data(item_limpo['data_publicacao'])
            
            # Validações
            item_limpo['link_valido'] = item_limpo['link'] and item_limpo['link'].startswith(('http://', 'https://'))
            item_limpo['tamanho_titulo'] = len(item_limpo['titulo']) if item_limpo['titulo'] else 0
            item_limpo['tamanho_descricao'] = len(item_limpo['descricao']) if item_limpo['descricao'] else 0
            
            self.dados_limpos.append(item_limpo)
        
        print(f"   ✅ Limpeza concluída: {len(self.dados_limpos)} registros limpos")
    
    def _formatar_data(self, data_rss):
        """Formata data do RSS para padrão brasileiro"""
        try:
            data_sem_tz = data_rss.rsplit(' ', 1)[0]
            data_obj = datetime.strptime(data_sem_tz, "%a, %d %b %Y %H:%M:%S")
            return data_obj.strftime("%d/%m/%Y %H:%M:%S")
        except:
            return data_rss

# Executar processo completo
scraper = G1ScraperCompleto()
sucesso_geral = scraper.executar_processo_completo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 3. Criação do Arquivo CSV\n",
    "\n",
    "Agora vamos criar o arquivo CSV final com todos os dados estruturados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_csv_final(dados, diretorio_saida):\n",
    "    \"\"\"\n",
    "    Cria o arquivo CSV final com os dados extraídos\n",
    "    \"\"\"\n    if not dados:\n        print(\"❌ Nenhum dado para criar CSV\")\n        return None\n    \n    print(\"📊 CRIANDO ARQUIVO CSV FINAL\")\n    print(\"-\" * 35)\n    \n    # Nome do arquivo com timestamp\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    nome_arquivo = f\"noticias_g1_brasil_{timestamp}.csv\"\n    caminho_arquivo = diretorio_saida / nome_arquivo\n    \n    # Definir campos do CSV\n    campos_csv = [\n        'id',\n        'titulo',\n        'data_publicacao',\n        'data_formatada', \n        'categoria',\n        'link',\n        'descricao',\n        'guid',\n        'link_valido',\n        'tamanho_titulo',\n        'tamanho_descricao',\n        'timestamp_extracao'\n    ]\n    \n    try:\n        print(f\"   📝 Criando arquivo: {nome_arquivo}\")\n        \n        with open(caminho_arquivo, 'w', newline='', encoding='utf-8') as arquivo:\n            writer = csv.DictWriter(arquivo, fieldnames=campos_csv)\n            \n            # Escrever cabeçalho\n            writer.writeheader()\n            \n            # Escrever dados\n            registros_escritos = 0\n            for item in dados:\n                # Preparar linha para CSV\n                linha_csv = {}\n                for campo in campos_csv:\n                    valor = item.get(campo, '')\n                    \n                    # Tratar valores especiais\n                    if valor is None:\n                        linha_csv[campo] = ''\n                    elif isinstance(valor, bool):\n                        linha_csv[campo] = 'Sim' if valor else 'Não'\n                    else:\n                        linha_csv[campo] = str(valor)\n                \n                writer.writerow(linha_csv)\n                registros_escritos += 1\n        \n        # Verificar arquivo criado\n        tamanho_arquivo = caminho_arquivo.stat().st_size\n        \n        print(f\"   ✅ CSV criado com sucesso!\")\n        print(f\"   📁 Local: {caminho_arquivo}\")\n        print(f\"   📊 Registros: {registros_escritos:,}\")\n        print(f\"   📏 Tamanho: {tamanho_arquivo:,} bytes\")\n        print(f\"   📋 Campos: {len(campos_csv)}\")\n        \n        return {\n            'caminho': str(caminho_arquivo),\n            'nome': nome_arquivo,\n            'registros': registros_escritos,\n            'tamanho_bytes': tamanho_arquivo,\n            'campos': campos_csv\n        }\n        \n    except Exception as e:\n        print(f\"   ❌ Erro ao criar CSV: {e}\")\n        return None\n\n# Criar CSV se temos dados\nif sucesso_geral and scraper.dados_limpos:\n    info_csv = criar_csv_final(scraper.dados_limpos, final_dir)\nelse:\n    info_csv = None\n    print(\"⚠️ Não foi possível criar CSV - verifique se o processo anterior foi bem-sucedido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 4. Relatório de Qualidade dos Dados\n",
    "\n",
    "Vamos gerar um relatório detalhado sobre a qualidade dos dados extraídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_relatorio_qualidade(dados, info_requisicao):\n    \"\"\"\n    Gera relatório detalhado de qualidade dos dados\n    \"\"\"\n    if not dados:\n        return None\n    \n    print(\"📋 RELATÓRIO DE QUALIDADE DOS DADOS\")\n    print(\"=\" * 45)\n    \n    total_registros = len(dados)\n    \n    # 1. Completude dos dados\n    print(\"\\n1️⃣ COMPLETUDE DOS DADOS:\")\n    campos_principais = ['titulo', 'link', 'descricao', 'data_publicacao', 'categoria']\n    \n    completude = {}\n    for campo in campos_principais:\n        completos = sum(1 for item in dados if item.get(campo) and str(item[campo]).strip())\n        percentual = (completos / total_registros) * 100\n        completude[campo] = {'completos': completos, 'percentual': percentual}\n        \n        status = \"✅\" if percentual >= 80 else \"⚠️\" if percentual >= 50 else \"❌\"\n        print(f\"   {status} {campo.capitalize()}: {completos}/{total_registros} ({percentual:.1f}%)\")\n    \n    # 2. Qualidade dos links\n    print(\"\\n2️⃣ QUALIDADE DOS LINKS:\")\n    links_validos = sum(1 for item in dados if item.get('link_valido', False))\n    perc_links = (links_validos / total_registros) * 100\n    status_links = \"✅\" if perc_links >= 90 else \"⚠️\" if perc_links >= 70 else \"❌\"\n    print(f\"   {status_links} Links válidos: {links_validos}/{total_registros} ({perc_links:.1f}%)\")\n    \n    # 3. Análise de tamanhos\n    print(\"\\n3️⃣ ANÁLISE DE TAMANHOS:\")\n    tamanhos_titulo = [item['tamanho_titulo'] for item in dados if item.get('tamanho_titulo', 0) > 0]\n    tamanhos_desc = [item['tamanho_descricao'] for item in dados if item.get('tamanho_descricao', 0) > 0]\n    \n    if tamanhos_titulo:\n        media_titulo = sum(tamanhos_titulo) / len(tamanhos_titulo)\n        print(f\"   📏 Títulos - Média: {media_titulo:.1f} chars, Min: {min(tamanhos_titulo)}, Max: {max(tamanhos_titulo)}\")\n    \n    if tamanhos_desc:\n        media_desc = sum(tamanhos_desc) / len(tamanhos_desc)\n        print(f\"   📏 Descrições - Média: {media_desc:.1f} chars, Min: {min(tamanhos_desc)}, Max: {max(tamanhos_desc)}\")\n    \n    # 4. Distribuição de categorias\n    print(\"\\n4️⃣ DISTRIBUIÇÃO DE CATEGORIAS:\")\n    categorias = {}\n    for item in dados:\n        cat = item.get('categoria') or 'Sem categoria'\n        categorias[cat] = categorias.get(cat, 0) + 1\n    \n    print(f\"   📂 Total de categorias únicas: {len(categorias)}\")\n    \n    # Top 5 categorias\n    top_categorias = sorted(categorias.items(), key=lambda x: x[1], reverse=True)[:5]\n    for i, (cat, count) in enumerate(top_categorias, 1):\n        perc = (count / total_registros) * 100\n        print(f\"   {i}. {cat}: {count} ({perc:.1f}%)\")\n    \n    # 5. Score geral de qualidade\n    print(\"\\n5️⃣ SCORE GERAL DE QUALIDADE:\")\n    \n    # Calcular score baseado em diferentes critérios\n    scores = {\n        'completude_titulo': completude['titulo']['percentual'],\n        'completude_link': completude['link']['percentual'], \n        'completude_descricao': completude['descricao']['percentual'],\n        'qualidade_links': perc_links,\n        'diversidade_categorias': min(100, len(categorias) * 10)  # Max 100\n    }\n    \n    score_final = sum(scores.values()) / len(scores)\n    \n    print(f\"   🏆 Score Final: {score_final:.1f}/100\")\n    \n    if score_final >= 80:\n        print(f\"   🌟 EXCELENTE! Dados de alta qualidade\")\n        qualidade = \"Excelente\"\n    elif score_final >= 60:\n        print(f\"   👍 BOM! Dados de qualidade adequada\")\n        qualidade = \"Boa\"\n    elif score_final >= 40:\n        print(f\"   ⚠️ REGULAR! Dados precisam de melhorias\")\n        qualidade = \"Regular\"\n    else:\n        print(f\"   ❌ BAIXA! Dados com problemas significativos\")\n        qualidade = \"Baixa\"\n    \n    # 6. Informações técnicas\n    print(\"\\n6️⃣ INFORMAÇÕES TÉCNICAS:\")\n    print(f\"   🌐 URL fonte: {info_requisicao.get('url', 'N/A')}\")\n    print(f\"   ⏱️ Tempo de captura: {info_requisicao.get('tempo_resposta', 0):.3f}s\")\n    print(f\"   📊 Tamanho original: {info_requisicao.get('tamanho_caracteres', 0):,} chars\")\n    print(f\"   🔤 Encoding: {info_requisicao.get('encoding', 'N/A')}\")\n    print(f\"   🕐 Processado em: {info_requisicao.get('timestamp', 'N/A')}\")\n    \n    return {\n        'score_final': score_final,\n        'qualidade': qualidade,\n        'completude': completude,\n        'total_registros': total_registros,\n        'categorias': categorias,\n        'scores_detalhados': scores\n    }\n\n# Gerar relatório se temos dados\nif sucesso_geral and scraper.dados_limpos:\n    relatorio_qualidade = gerar_relatorio_qualidade(scraper.dados_limpos, scraper.info_requisicao)\nelse:\n    relatorio_qualidade = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 5. Visualizações dos Dados\n",
    "\n",
    "Vamos criar algumas visualizações simples usando caracteres para mostrar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_visualizacoes_dados(dados, relatorio):\n    \"\"\"\n    Cria visualizações simples dos dados usando caracteres\n    \"\"\"\n    if not dados or not relatorio:\n        return\n    \n    print(\"📈 VISUALIZAÇÕES DOS DADOS\")\n    print(\"=\" * 35)\n    \n    # 1. Gráfico de completude\n    print(\"\\n1️⃣ GRÁFICO DE COMPLETUDE DOS CAMPOS:\")\n    print(\"-\" * 40)\n    \n    for campo, info in relatorio['completude'].items():\n        percentual = info['percentual']\n        barra_tamanho = int(percentual / 5)  # Cada char = 5%\n        barra = \"█\" * barra_tamanho + \"░\" * (20 - barra_tamanho)\n        \n        print(f\"{campo.capitalize():12} |{barra}| {percentual:5.1f}%\")\n    \n    # 2. Distribuição de categorias (top 10)\n    print(\"\\n2️⃣ DISTRIBUIÇÃO DE CATEGORIAS (TOP 10):\")\n    print(\"-\" * 45)\n    \n    categorias = relatorio['categorias']\n    top_categorias = sorted(categorias.items(), key=lambda x: x[1], reverse=True)[:10]\n    max_count = max([count for _, count in top_categorias]) if top_categorias else 1\n    \n    for i, (categoria, count) in enumerate(top_categorias, 1):\n        # Truncar categoria se muito longa\n        cat_display = categoria[:15] + \"...\" if len(categoria) > 18 else categoria\n        \n        # Criar barra proporcional\n        barra_tamanho = int((count / max_count) * 25)\n        barra = \"▓\" * barra_tamanho\n        \n        percentual = (count / relatorio['total_registros']) * 100\n        print(f\"{i:2d}. {cat_display:18} |{barra:<25}| {count:3d} ({percentual:4.1f}%)\")\n    \n    # 3. Indicadores de qualidade\n    print(\"\\n3️⃣ INDICADORES DE QUALIDADE:\")\n    print(\"-\" * 35)\n    \n    indicadores = [\n        (\"Score Geral\", relatorio['score_final'], 100),\n        (\"Completude Títulos\", relatorio['completude']['titulo']['percentual'], 100),\n        (\"Completude Links\", relatorio['completude']['link']['percentual'], 100),\n        (\"Qualidade Links\", relatorio['scores_detalhados']['qualidade_links'], 100),\n        (\"Diversidade Cats\", relatorio['scores_detalhados']['diversidade_categorias'], 100)\n    ]\n    \n    for nome, valor, maximo in indicadores:\n        percentual = (valor / maximo) * 100\n        \n        # Escolher cor/símbolo baseado no valor\n        if percentual >= 80:\n            simbolo, cor = \"🟢\", \"ÓTIMO\"\n        elif percentual >= 60:\n            simbolo, cor = \"🟡\", \"BOM\"\n        elif percentual >= 40:\n            simbolo, cor = \"🟠\", \"REGULAR\"\n        else:\n            simbolo, cor = \"🔴\", \"BAIXO\"\n        \n        barra_tamanho = int(percentual / 5)\n        barra = \"█\" * barra_tamanho + \"░\" * (20 - barra_tamanho)\n        \n        print(f\"{simbolo} {nome:15} |{barra}| {valor:5.1f} ({cor})\")\n    \n    # 4. Timeline das publicações (se tivermos datas)\n    print(\"\\n4️⃣ RESUMO TEMPORAL:\")\n    print(\"-\" * 25)\n    \n    datas_validas = []\n    for item in dados:\n        if item.get('data_formatada'):\n            try:\n                # Tentar extrair apenas a data\n                data_str = item['data_formatada'].split(' ')[0]\n                datas_validas.append(data_str)\n            except:\n                pass\n    \n    if datas_validas:\n        datas_unicas = list(set(datas_validas))\n        print(f\"   📅 Período: {len(datas_unicas)} dias únicos\")\n        print(f\"   📊 Notícias/dia: {len(dados) / len(datas_unicas):.1f} (média)\")\n        \n        # Contar por data\n        contador_datas = {}\n        for data in datas_validas:\n            contador_datas[data] = contador_datas.get(data, 0) + 1\n        \n        # Mostrar top 5 dias\n        top_dias = sorted(contador_datas.items(), key=lambda x: x[1], reverse=True)[:5]\n        print(f\"\\n   🏆 TOP 5 DIAS COM MAIS NOTÍCIAS:\")\n        for i, (data, count) in enumerate(top_dias, 1):\n            print(f\"   {i}. {data}: {count} notícias\")\n    else:\n        print(\"   ⚠️ Nenhuma data válida encontrada para análise temporal\")\n\n# Criar visualizações se temos dados\nif relatorio_qualidade:\n    criar_visualizacoes_dados(scraper.dados_limpos, relatorio_qualidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 6. Exportação em Múltiplos Formatos\n",
    "\n",
    "Vamos exportar os dados em diferentes formatos para máxima compatibilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportar_multiplos_formatos(dados, info_requisicao, relatorio_qualidade, diretorio):\n    \"\"\"\n    Exporta dados em múltiplos formatos\n    \"\"\"\n    if not dados:\n        return {}\n    \n    print(\"💾 EXPORTANDO EM MÚLTIPLOS FORMATOS\")\n    print(\"-\" * 40)\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    arquivos_gerados = {}\n    \n    # 1. JSON detalhado\n    print(\"   📄 Criando JSON detalhado...\")\n    try:\n        nome_json = f\"dados_completos_{timestamp}.json\"\n        caminho_json = diretorio / nome_json\n        \n        dados_json = {\n            'metadados': {\n                'fonte': 'G1 RSS Brasil',\n                'url': info_requisicao.get('url'),\n                'timestamp_extracao': info_requisicao.get('timestamp'),\n                'total_registros': len(dados),\n                'qualidade_dados': relatorio_qualidade['qualidade'] if relatorio_qualidade else 'N/A',\n                'score_qualidade': relatorio_qualidade['score_final'] if relatorio_qualidade else 0,\n                'gerado_por': 'Notebook 4 - Scraping G1'\n            },\n            'dados': dados,\n            'estatisticas': relatorio_qualidade if relatorio_qualidade else {},\n            'info_tecnica': info_requisicao\n        }\n        \n        with open(caminho_json, 'w', encoding='utf-8') as arquivo:\n            json.dump(dados_json, arquivo, ensure_ascii=False, indent=2)\n        \n        arquivos_gerados['json'] = {\n            'nome': nome_json,\n            'caminho': str(caminho_json),\n            'tamanho': caminho_json.stat().st_size\n        }\n        print(f\"   ✅ JSON: {nome_json} ({arquivos_gerados['json']['tamanho']:,} bytes)\")\n        \n    except Exception as e:\n        print(f\"   ❌ Erro no JSON: {e}\")\n    \n    # 2. TXT relatório\n    print(\"   📄 Criando relatório TXT...\")\n    try:\n        nome_txt = f\"relatorio_scraping_{timestamp}.txt\"\n        caminho_txt = diretorio / nome_txt\n        \n        with open(caminho_txt, 'w', encoding='utf-8') as arquivo:\n            arquivo.write(\"=\" * 60 + \"\\n\")\n            arquivo.write(\"RELATÓRIO DE SCRAPING - G1 RSS BRASIL\\n\")\n            arquivo.write(\"Gerado pelos Notebooks 1-4\\n\")\n            arquivo.write(\"=\" * 60 + \"\\n\\n\")\n            \n            # Informações gerais\n            arquivo.write(\"INFORMAÇÕES GERAIS:\\n\")\n            arquivo.write(\"-\" * 20 + \"\\n\")\n            arquivo.write(f\"URL fonte: {info_requisicao.get('url', 'N/A')}\\n\")\n            arquivo.write(f\"Data/hora extração: {info_requisicao.get('timestamp', 'N/A')}\\n\")\n            arquivo.write(f\"Total de notícias: {len(dados)}\\n\")\n            arquivo.write(f\"Tempo de resposta: {info_requisicao.get('tempo_resposta', 0):.3f}s\\n\")\n            \n            if relatorio_qualidade:\n                arquivo.write(f\"\\nQUALIDADE DOS DADOS:\\n\")\n                arquivo.write(\"-\" * 20 + \"\\n\")\n                arquivo.write(f\"Score geral: {relatorio_qualidade['score_final']:.1f}/100\\n\")\n                arquivo.write(f\"Classificação: {relatorio_qualidade['qualidade']}\\n\")\n                \n                arquivo.write(f\"\\nCOMPLETUDE POR CAMPO:\\n\")\n                for campo, info in relatorio_qualidade['completude'].items():\n                    arquivo.write(f\"  {campo}: {info['percentual']:.1f}%\\n\")\n                \n                arquivo.write(f\"\\nTOP 10 CATEGORIAS:\\n\")\n                categorias = relatorio_qualidade['categorias']\n                top_cats = sorted(categorias.items(), key=lambda x: x[1], reverse=True)[:10]\n                for i, (cat, count) in enumerate(top_cats, 1):\n                    perc = (count / len(dados)) * 100\n                    arquivo.write(f\"  {i:2d}. {cat}: {count} ({perc:.1f}%)\\n\")\n            \n            arquivo.write(f\"\\nPRIMEIRAS 5 NOTÍCIAS (AMOSTRA):\\n\")\n            arquivo.write(\"-\" * 35 + \"\\n\")\n            for i, item in enumerate(dados[:5], 1):\n                arquivo.write(f\"\\n{i}. {item.get('titulo', 'N/A')}\\n\")\n                arquivo.write(f\"   Data: {item.get('data_formatada', 'N/A')}\\n\")\n                arquivo.write(f\"   Categoria: {item.get('categoria', 'N/A')}\\n\")\n                arquivo.write(f\"   Link: {item.get('link', 'N/A')[:60]}...\\n\")\n            \n            arquivo.write(f\"\\n\\nFIM DO RELATÓRIO\\n\")\n        \n        arquivos_gerados['txt'] = {\n            'nome': nome_txt,\n            'caminho': str(caminho_txt),\n            'tamanho': caminho_txt.stat().st_size\n        }\n        print(f\"   ✅ TXT: {nome_txt} ({arquivos_gerados['txt']['tamanho']:,} bytes)\")\n        \n    except Exception as e:\n        print(f\"   ❌ Erro no TXT: {e}\")\n    \n    return arquivos_gerados\n\n# Exportar se temos dados\nif sucesso_geral and scraper.dados_limpos:\n    arquivos_exportados = exportar_multiplos_formatos(\n        scraper.dados_limpos, \n        scraper.info_requisicao, \n        relatorio_qualidade, \n        final_dir\n    )\nelse:\n    arquivos_exportados = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 7. Relatório Final Completo\n",
    "\n",
    "Vamos gerar o relatório final de todo o projeto dos 4 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 RELATÓRIO FINAL COMPLETO DO PROJETO\")\nprint(\"=\" * 50)\n\n# Tempo total de execução\ntempo_total = datetime.now() - scraper.timestamp_inicio\n\nprint(f\"⏱️ TEMPO DE EXECUÇÃO: {tempo_total.total{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Notebook 4: Criar CSV com Dados Finais\n",
    "\n",
    "**Objetivo**: Criar arquivo CSV final com todos os dados extraídos e estruturados\n",
    "\n",
    "**O que vamos fazer:**\n",
    "1. 🔄 Consolidar todo o processo (Notebooks 1-3)\n",
    "2. 📊 Criar CSV com dados estruturados\n",
    "3. 📋 Gerar relatório final de qualidade\n",
    "4. 📈 Criar visualizações dos dados\n",
    "5. 💾 Exportar múltiplos formatos\n",
    "6. 🎯 Relatório completo do projeto\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 1. Configuração Final e Importações\n",
    "\n",
    "Vamos importar tudo que precisamos para finalizar nosso projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações completas\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"🎉 NOTEBOOK 4 - CRIAÇÃO DO CSV FINAL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Todas as bibliotecas importadas!\")\n",
    "print(f\"🕐 Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Configurar diretórios\n",
    "output_dir = Path(\"notebook_outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "final_dir = Path(\"dados_finais\")\n",
    "final_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"📁 Diretório de trabalho: {output_dir}\")\n",
    "print(f\"📁 Diretório final: {final_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 2. Processo Completo Consolidado\n",
    "\n",
    "Vamos executar todo o processo de uma vez, consolidando os Notebooks 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G1ScraperCompleto:\n",
    "    \"\"\"\n",
    "    Classe que consolida todo o processo dos notebooks anteriores\n",
    "    \"\"\"\n    \n    def __init__(self):\n        self.url = \"https://g1.globo.com/rss/g1/brasil/\"\n        self.dados_brutos = None\n        self.dados_limpos = []\n        self.info_requisicao = {}\n        self.estatisticas = {}\n        self.timestamp_inicio = datetime.now()\n    \n    def executar_processo_completo(self):\n        \"\"\"\n        Executa todo o processo do scraping\n        \"\"\"\n        print(\"🚀 INICIANDO PROCESSO COMPLETO DE SCRAPING\")\n        print(\"-\" * 45)\n        \n        # Etapa 1: Requisição (Notebook 1)\n        print(\"\\n📡 ETAPA 1: REQUISIÇÃO E CAPTURA\")\n        sucesso = self._fazer_requisicao()\n        if not sucesso:\n            return False\n        \n        # Etapa 2: Análise da estrutura (Notebook 2)\n        print(\"\\n📊 ETAPA 2: ANÁLISE DA ESTRUTURA\")\n        sucesso = self._analisar_estrutura()\n        if not sucesso:\n            return False\n        \n        # Etapa 3: Parsing e extração (Notebook 3)\n        print(\"\\n🔍 ETAPA 3: PARSING E EXTRAÇÃO\")\n        sucesso = self._parsear_e_extrair()\n        if not sucesso:\n            return False\n        \n        # Etapa 4: Limpeza e preparação\n        print(\"\\n🧹 ETAPA 4: LIMPEZA E PREPARAÇÃO\")\n        self._limpar_e_preparar()\n        \n        print(\"\\n✅ PROCESSO COMPLETO FINALIZADO!\")\n        return True\n    \n    def _fazer_requisicao(self):\n        \"\"\"Etapa 1: Fazer requisição HTTP\"\"\"\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Accept': 'application/rss+xml, application/xml, text/xml'\n        }\n        \n        try:\n            print(f\"   🔄 Requisição para: {self.url}\")\n            response = requests.get(self.url, headers=headers, timeout=15)\n            response.raise_for_status()\n            \n            self.dados_brutos = response.text\n            self.info_requisicao = {\n                'url': self.url,\n                'status_code': response.status_code,\n                'tamanho_bytes': len(response.content),\n                'tamanho_caracteres': len(response.text),\n                'encoding': response.encoding,\n                'tempo_resposta': response.elapsed.total_seconds(),\n                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            }\n            \n            print(f\"   ✅ Sucesso! {len(self.dados_brutos):,} caracteres capturados\")\n            return True\n            \n        except requests.exceptions.RequestException as e:\n            print(f\"   ❌ Erro na requisição: {e}\")\n            return False\n    \n    def _analisar_estrutura(self):\n        \"\"\"Etapa 2: Analisar estrutura do RSS\"\"\"\n        try:\n            # Contar elementos principais\n            elementos = {\n                'items': self.dados_brutos.count('<item>'),\n                'titles': self.dados_brutos.count('<title>'),\n                'links': self.dados_brutos.count('<link>'),\n                'descriptions': self.dados_brutos.count('<description>'),\n                'categories': self.dados_brutos.count('